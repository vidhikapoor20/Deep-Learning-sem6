{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bottleneck.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXqHnQ7EaR0Z"
      },
      "source": [
        "#Transfer Learning vs Fine-tuning\n",
        "\n",
        "The pre-trained models are trained on very large scale image classification problems. The convolutional layers act as feature extractor and the fully connected layers act as Classifiers.\n",
        "\n",
        "![Image](http://upscfever.com/upsc-fever/en/data/images/cnn-schema1.jpg)\n",
        "\n",
        "Since these models are very large and have seen a huge number of images, they tend to learn very good, discriminative features. We can either use the convolutional layers merely as a feature extractor or we can tweak the already trained convolutional layers to suit our problem at hand. The former approach is known as Transfer Learning and the latter as Fine-tuning.\n",
        "\n",
        "As a rule of thumb, when we have a small training set and our problem is similar to the task for which the pre-trained models were trained, we can use transfer learning. If we have enough data, we can try and tweak the convolutional layers so that they learn more robust features relevant to our problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYkuVZh7bLqx"
      },
      "source": [
        "#Transfer Learning\n",
        "\n",
        "In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows:\n",
        "\n",
        "ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.\n",
        "\n",
        "#Fine-tuning the ConvNet. \n",
        "\n",
        "The second strategy is to not only replace and retrain the classifier on top of the ConvNet on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the backpropagation. It is possible to fine-tune all the layers of the ConvNet, or it’s possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. This is motivated by the observation that the earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the ConvNet becomes progressively more specific to the details of the classes contained in the original dataset. In case of ImageNet for example, which contains many dog breeds, a significant portion of the representational power of the ConvNet may be devoted to features that are specific to differentiating between dog breeds.\n",
        "\n",
        "#Pretrained models. \n",
        "\n",
        "Since modern ConvNets take 2-3 weeks to train across multiple GPUs on ImageNet, it is common to see people release their final ConvNet checkpoints for the benefit of others who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo where people share their network weights.\n",
        "When and how to fine-tune? How do you decide what type of transfer learning you should perform on a new dataset? This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different, such as microscope images). Keeping in mind that ConvNet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios:\n",
        "\n",
        "New dataset is small and similar to original dataset. Since the data is small, it is not a good idea to fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes.\n",
        "New dataset is large and similar to the original dataset. Since we have more data, we can have more confidence that we won’t overfit if we were to try to fine-tune through the full network.\n",
        "New dataset is small but very different from the original dataset. Since the data is small, it is likely best to only train a linear classifier. Since the dataset is very different, it might not be best to train the classifier form the top of the network, which contains more dataset-specific features. Instead, it might work better to train the SVM classifier from activations somewhere earlier in the network.\n",
        "New dataset is large and very different from the original dataset. Since the dataset is very large, we may expect that we can afford to train a ConvNet from scratch. However, in practice it is very often still beneficial to initialize with weights from a pretrained model. In this case, we would have enough data and confidence to fine-tune through the entire network.\n",
        "Practical advice. There are a few additional things to keep in mind when performing Transfer Learning:\n",
        "\n",
        "Constraints from pretrained models. Note that if you wish to use a pretrained network, you may be slightly constrained in terms of the architecture you can use for your new dataset. For example, you can’t arbitrarily take out Conv layers from the pretrained network. However, some changes are straight-forward: Due to parameter sharing, you can easily run a pretrained network on images of different spatial size. This is clearly evident in the case of Conv/Pool layers because their forward function is independent of the input volume spatial size (as long as the strides “fit”). In case of FC layers, this still holds true because FC layers can be converted to a Convolutional Layer: For example, in an AlexNet, the final pooling volume before the first FC layer is of size [6x6x512]. Therefore, the FC layer looking at this volume is equivalent to having a Convolutional Layer that has receptive field size 6x6, and is applied with padding of 0.\n",
        "\n",
        "Learning rates. It’s common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don’t wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-kc6xzaNdV"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2\n",
        "import PIL.Image\n",
        "import urllib\n",
        "page = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n04194289\")#ship synset\n",
        "print(page.content)\n",
        "# BeautifulSoup is an HTML parsing library\n",
        "soup = BeautifulSoup(page.content, 'html.parser')#puts the content of the website into the soup variable, each url on a different line\n",
        "\n",
        "bikes_page = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02834778\")#bicycle synset\n",
        "print(bikes_page.content)\n",
        "# BeautifulSoup is an HTML parsing library\n",
        "from bs4 import BeautifulSoup\n",
        "bikes_soup = BeautifulSoup(bikes_page.content, 'html.parser')#puts the content of the website into the soup variable, each url on a different line\n",
        "\n",
        "str_soup=str(soup)#convert soup to string so it can be split\n",
        "type(str_soup)\n",
        "split_urls=str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
        "print(len(split_urls))#print the length of the list so you know how many urls you have\n",
        "\n",
        "bikes_str_soup=str(bikes_soup)#convert soup to string so it can be split\n",
        "type(bikes_str_soup)\n",
        "bikes_split_urls=bikes_str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
        "print(len(bikes_split_urls))\n",
        "\n",
        "!mkdir /content/train #create the Train folder\n",
        "!mkdir /content/train/ships #create the ships folder\n",
        "!mkdir /content/train/bikes #create the bikes folder\n",
        "!mkdir /content/validation\n",
        "!mkdir /content/validation/ships #create the ships folder\n",
        "!mkdir /content/validation/bikes #create the bikes folder\n",
        "\n",
        "#code part 4\n",
        "img_rows, img_cols = 32, 32 #number of rows and columns to convert the images to\n",
        "input_shape = (img_rows, img_cols, 3)#format to store the images (rows, columns,channels) called channels last\n",
        "\n",
        "def url_to_image(url):\n",
        "\t# download the image, convert it to a NumPy array, and then read\n",
        "\t# it into OpenCV format\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\timage = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\timage = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        " \n",
        "\t# return the image\n",
        "\treturn image\n",
        "\n",
        "n_of_training_images=100#the number of training images to use\n",
        "for progress in range(n_of_training_images):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(split_urls[progress])\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/train/ships/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "#do the same for bikes:\n",
        "for progress in range(n_of_training_images):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not bikes_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(bikes_split_urls[progress])\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/train/bikes/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "        \n",
        "        \n",
        "#Validation data:\n",
        "\n",
        "for progress in range(50):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(split_urls[n_of_training_images+progress])#get images that are different from the ones used for training\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/validation/ships/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "#do the same for bikes:\n",
        "for progress in range(50):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not bikes_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(bikes_split_urls[n_of_training_images+progress])#get images that are different from the ones used for training\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/validation/bikes/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "        \n",
        "print(\"\\nTRAIN:\\n\")          \n",
        "print(\"\\nlist the files inside ships directory:\\n\")        \n",
        "!ls /content/train/ships #list the files inside ships\n",
        "print(\"\\nlist the files inside bikes directory:\\n\")\n",
        "!ls /content/train/bikes #list the files inside bikes\n",
        "print(\"\\nVALIDATION:\\n\")\n",
        "print(\"\\nlist the files inside ships directory:\\n\")        \n",
        "!ls /content/validation/ships #list the files inside ships\n",
        "print(\"\\nlist the files inside bikes directory:\\n\")\n",
        "!ls /content/validation/bikes #list the files inside bikes   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YdqAb33hSeI"
      },
      "source": [
        "#Data pre-processing and data augmentation\n",
        "\n",
        "In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better.\n",
        "\n",
        "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class. This class allows you to:\n",
        "\n",
        "1. configure random transformations and normalization operations to be done on your image data during training\n",
        "2. instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs, fit_generator, evaluate_generator and predict_generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPjFTAkcfhxl"
      },
      "source": [
        "#Load the pre-trained model VGG16\n",
        "\n",
        "Our strategy will be as follow: we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training and validation data once, recording the output (the \"bottleneck features\" from th VGG16 model: the last activation maps before the fully-connected layers) in two numpy arrays. Then we will train a small fully-connected model on top of the stored features.\n",
        "\n",
        "![alt text](http://upscfever.com/upsc-fever/en/data/images/vgg16_original.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krtgdSuwNKOJ"
      },
      "source": [
        "#Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH9Og-1cOYIJ"
      },
      "source": [
        "import cv2                  \r\n",
        "import numpy as np  \r\n",
        "from tqdm import tqdm\r\n",
        "import os                   \r\n",
        "from random import shuffle  \r\n",
        "from zipfile import ZipFile\r\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD8FryeiNLk5"
      },
      "source": [
        "X = []\r\n",
        "y = []\r\n",
        "train_bikes = '/content/train/bikes'\r\n",
        "train_ships = '/content/train/ships'\r\n",
        "val_bikes = '/content/validation/bikes'\r\n",
        "val_ships = '/content/validation/ships'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDsEMwTNNmwv"
      },
      "source": [
        "def assign_label(img,flower_type):\r\n",
        "    return flower_type\r\n",
        "\r\n",
        "def make_train_data(flower_type,DIR):\r\n",
        "    for img in tqdm(os.listdir(DIR)):\r\n",
        "        label=assign_label(img,flower_type)\r\n",
        "        path = os.path.join(DIR,img)\r\n",
        "        img = cv2.imread(path,cv2.IMREAD_COLOR)           \r\n",
        "        X.append(np.array(img))\r\n",
        "        y.append(str(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXnm5j6NwCA"
      },
      "source": [
        "make_train_data('bikes', train_bikes)\r\n",
        "make_train_data('ships', train_ships)\r\n",
        "make_train_data('bikes', val_bikes)\r\n",
        "make_train_data('ships', val_ships)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg3jujj9OayP"
      },
      "source": [
        "len(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta5t_Bh0OhBN"
      },
      "source": [
        "len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCNsvnTwOi79"
      },
      "source": [
        "Xarray = np.array(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_4SgAnmO7cZ"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG9xo3r8Ory9"
      },
      "source": [
        "le=LabelEncoder()\r\n",
        "Y=le.fit_transform(y)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv_I10-qPF_9"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eays_SD_O82d"
      },
      "source": [
        "Xarray=Xarray/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJDr2rD3O_St"
      },
      "source": [
        "Xarray[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bPV24tYPL-u"
      },
      "source": [
        "Xarray.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kob35S3qLlxx"
      },
      "source": [
        "import numpy as np\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dropout, Flatten, Dense\r\n",
        "from keras import applications\r\n",
        "from keras.layers import Input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoMSnWdOMYyO"
      },
      "source": [
        "model = applications.VGG16(include_top=False, weights='imagenet', input_tensor=Input(shape=(32,32,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Nd1Cj7M3nI"
      },
      "source": [
        "bottleneck_features = model.predict()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}